{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set inline plot for jupyter ipynb\n",
    "% matplotlib inline\n",
    "\n",
    "# import the modules\n",
    "from time import time\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords\n",
    "from six import iteritems\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, re, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# use peter-thompson/Topic-modelling-using-LDA Pre-processing functions\n",
    "def re_clean_list():\n",
    "    # define a list of regular expression clean up to use\n",
    "    re_list = []\n",
    "    re_list.append(re.compile('>'))\n",
    "    re_list.append(re.compile('(Message-ID(.*?\\n)*X-FileName.*?\\n)|'\n",
    "                     '(To:(.*?\\n)*?Subject.*?\\n)|'\n",
    "                     '(< (Message-ID(.*?\\n)*.*?X-FileName.*?\\n))'))\n",
    "    re_list.append(re.compile('(.+)@(.+)')) # Remove emails\n",
    "    re_list.append(re.compile('\\s(-----)(.*?)(-----)\\s', re.DOTALL))\n",
    "    re_list.append(re.compile('''\\s(\\*\\*\\*\\*\\*)(.*?)(\\*\\*\\*\\*\\*)\\s''', re.DOTALL))\n",
    "    re_list.append(re.compile('\\s(_____)(.*?)(_____)\\s', re.DOTALL))\n",
    "    re_list.append(re.compile('\\n( )*-.*'))\n",
    "    re_list.append(re.compile('\\n( )*\\d.*'))\n",
    "    re_list.append(re.compile('(\\n( )*[\\w]+($|( )*\\n))|(\\n( )*(\\w)+(\\s)+(\\w)+(( )*\\n)|$)|(\\n( )*(\\w)+(\\s)+(\\w)+(\\s)+(\\w)+(( )*\\n)|$)'))\n",
    "    re_list.append(re.compile('.*orwarded.*'))\n",
    "    re_list.append(re.compile('From.*|Sent.*|cc.*|Subject.*|Embedded.*|http.*|\\w+\\.\\w+|.*\\d\\d/\\d\\d/\\d\\d\\d\\d.*'))\n",
    "    re_list.append(re.compile(' [\\d:;,.]+ '))\n",
    "    return re_list\n",
    "\n",
    "def remove_clutter(text,re_list):\n",
    "    # takes in the regular expression to remove clutter\n",
    "    for i in range(len(re_list)):\n",
    "        text = re.sub(re_list[i], ' ', text)\n",
    "    return text\n",
    "\"\"\"\n",
    "\n",
    "# ============================\n",
    "# regular expression patterns\n",
    "# ============================\n",
    "# pattern to extract data into structure\n",
    "date_pattern = re.compile('(?<=Date:).*',re.IGNORECASE)\n",
    "msgId_pattern = re.compile('(?<=: <)(.*?)(?=@)',re.IGNORECASE)\n",
    "from_pattern = re.compile(r\"(?<=From: )[\\\"']*(.*?)(?:\\\"|\\'|\\n|\\<|\\[|\\\\)\",re.IGNORECASE) #note: still have to clean up names\n",
    "sent_pattern = re.compile('(?<=Sent: ).*',re.IGNORECASE)\n",
    "to_pattern = re.compile(r\"(?<=To: )[\\\"']*(.*?)(?:\\\"|\\'|\\n|\\<|\\[|\\\\)\",re.IGNORECASE)\n",
    "subj_pattern = re.compile('(?<=Subject: )[Fw: | Fwd]*(.*)',re.IGNORECASE)\n",
    "clean_content_pattern = re.compile(r'[\\W\\d]\\s*')\n",
    "content_pattern = re.compile(r\"[\\t|\\n]*(.*)[\\n]\") # only can use with clear patterns below\n",
    "\n",
    "# patterns to clean up data\n",
    "# clear_content_pattern = re.compile('(?:.*:)(.*)')\n",
    "clear_header_pattern = re.compile('(?:from:.*|mime-.*|sent.*|to:.*|'\n",
    "                                      'subject:.*|received:.*|date:.*|'\n",
    "                                      'folder:.*|filename:.*|cc:.*|Message-ID:.*|'\n",
    "                                      'X-.*|status:.*|content-.*|'\n",
    "                                      'boundary[-|=].*|http.*)',re.IGNORECASE)\n",
    "clear_star_pattern = re.compile('(?<=\\*)[\\n].*[\\n](?=\\*)')\n",
    "clear_symbol_pattern = re.compile('[\\*|\\-|\\=|\\_]{2,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# own pre-processing functions\n",
    "# ============================\n",
    "def uniq_set(list_item):\n",
    "    return list(set(list_item))\n",
    "\n",
    "def clean_names(list_item):\n",
    "    list_item = [i.strip() for i in list_item]\n",
    "    # list_item = [re.findall(parse_name_pattern,i)[0].strip() for i in list_item]\n",
    "    return uniq_set(list_item)\n",
    "\n",
    "def parse_data(content):\n",
    "    # this function takes in the data and parses it and stores into\n",
    "    # a json object for retrival\n",
    "    c_data = {} # stores the content data\n",
    "    extract_date = re.findall(date_pattern,content)\n",
    "    extract_sent = re.findall(sent_pattern,content)\n",
    "    extract_date.extend(extract_sent)\n",
    "    # store some of the key informaion\n",
    "    c_data['isValid'] = True # set data to valid\n",
    "    c_data['date'] = uniq_set(extract_date)\n",
    "    c_data['msgId'] = re.findall(msgId_pattern,content)\n",
    "    c_data['from'] = clean_names(re.findall(from_pattern,content))\n",
    "    c_data['to'] = clean_names(re.findall(to_pattern,content))\n",
    "    c_data['subj'] = uniq_set(re.findall(subj_pattern,content))\n",
    "    c_data['file'] = ''\n",
    "    # extract contents from the email\n",
    "    remove_idx = content.find('\\n_____') # remove everything after this\n",
    "    content = content[0:remove_idx]\n",
    "    remove_idx = content.find('\\n*******')\n",
    "    content = content[0:remove_idx]\n",
    "    content = re.sub(clear_header_pattern,'',content)\n",
    "    # content = re.sub(clear_star_pattern,'',content)\n",
    "    content = re.sub(clear_symbol_pattern,'',content)\n",
    "    content = re.findall(content_pattern,content)\n",
    "    c_data['content'] = [' '.join(content)]\n",
    "    if c_data['content'][0] == '': # if there is no content\n",
    "        c_data['isValid'] = False # set the validity to false\n",
    "    else:\n",
    "        # final clean\n",
    "        c_data['content'][0] = re.sub(clean_content_pattern,' ',c_data['content'][0])\n",
    "    # return the data\n",
    "    return c_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved file ...\n",
      "Data extracted and saved in raw_doc\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Extract the Enron data\n",
    "# ======================\n",
    "# change this directory to point to where the Enron data is\n",
    "root_dir = '/home/hweilun/data/corpora/Enron/extract'\n",
    "\n",
    "# check whether to store, overwrite or open pickled file\n",
    "saved_file = \"raw_doc.p\"\n",
    "if saved_file in os.listdir('.'):\n",
    "    # open the pickled file\n",
    "    print 'Loading saved file ...'\n",
    "    raw_doc = pickle.load(open(saved_file,\"rb\"))\n",
    "else:\n",
    "    # structure to store the original contents\n",
    "    print 'Reading and extracting ...'\n",
    "    raw_doc = []\n",
    "    # re_list = re_clean_list()\n",
    "    start = time()\n",
    "    for sub_dir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            # obtain the file name to load\n",
    "            # only want to open files with NO extension!\n",
    "            if file.find('.') == -1:\n",
    "                file_path = os.path.join(sub_dir, file)\n",
    "                f=open(file_path,'r')\n",
    "                content = f.read()\n",
    "                # content = remove_clutter(content,re_list)\n",
    "                # content = f.readlines()\n",
    "                # print content\n",
    "                extract_data = parse_data(content)\n",
    "                extract_data['file'] = file_path # store the file path\n",
    "                if extract_data['isValid']:\n",
    "                    raw_doc.append(extract_data['content'][0])\n",
    "    end = time()           \n",
    "    print 'Finish extraction, time taken = ',(end-start)\n",
    "    # save the file\n",
    "    pickle.dump(raw_doc, open(\"raw_doc.p\",\"wb\"))\n",
    "print 'Data extracted and saved in raw_doc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop words\n",
    "custom_stopwords = ['is','a','on','at','the','and','to','i','be']\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "stoplist= set(nltk_stopwords + custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0a39480da18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# tokens = [[word for word in doc.lower().split() if word not in stoplist and len(word)>1] for doc in raw_doc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq_stopwords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# extract tokens from the documents\n",
    "start = time()\n",
    "# tokens = [[word for word in doc.lower().split() if word not in stoplist and len(word)>1] for doc in raw_doc]\n",
    "tokens = [[word for word in doc.lower().split() if word not in freq_stopwords and len(word)>1] for doc in raw_doc]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "end = time()\n",
    "print 'Tokens, dict and corpus loaded in ',(end-start)\n",
    "\"\"\"\n",
    "start = time()\n",
    "tokens = [[word for word in doc.lower().split()] for doc in raw_doc]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "# stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "freq_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "freq_stopwords = [dictionary[ids] for ids in freq_ids]\n",
    "stoplist = set(nltk_stopwords + custom_stopwords + freq_stopwords)\n",
    "tokens = [[word for word in doc.lower().split() if word not in stoplist and len(word)>1] for doc in raw_doc]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(tokens) for token in tokens]\n",
    "end = time()\n",
    "print 'Tokens, dict and corpus loaded in ',(end-start)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'vladi_pimenov', u'lrc_bridgeline', u'stephanie_panus', u'ene_ect', u'elizabeth_sager', u'technote', u'eesiisonewyork', u'jane_tholt', u'eric_linder_', u'aapending', u'lynn_blair_', u'joseph_parks_nov', u'sandra_brawner', u'mbamarketing', u'john_griffith_jan', u'phillip_love_jan', u'chris_dorland_dec', u'houstonfundamentals', u'joseph_quenet_dec', u'richard_sanders_jan', u'steven_merriss', u'sally_beck_', u'barry_tycholiz_mar', u'bradley_mckay_jun', u'jonathan_mckay_', u'larry_campbell_jun', u'bill_williams_iii', u'daren_farmer', u'techmemos', u'john_arnold_', u'jason_wolfe_nov', u'holden_salisbury_', u'sparefinders', u'dana_davis_dec', u'robin_rodrigue', u'david_delainey_', u'vladi_pimenov_', u'james_schwieger_nov', u'stacey_white_mar', u'sally_beck_nov', u'phillip_platter', u'john_hodge_', u'joseph_quenet_jun', u'randall_gay_jan', u'directaccess', u'james_steffes_', u'john_zufferlie_jun', u'jeff_dasovich_', u'errol_mclaughlin_mar', u'sally_beck_dec', u'judy_townsend', u'mangmt', u'gerald_nemec_nov', u'michael_grigsby_jun', u'andrew_lewis_', u'drew_fossum_', u'michael_maggi_mar', u'clint_dean_jun', u'dana_davis', u'timbelden', u'carol_stclair_june', u'venezuela_colombia', u'carol_stclair_nov', u'louise_kitchen_jan', u'sara_shackleton_', u'phillip_allen', u'info_announcements', u'darron_giron_jan', u'ryan_slinger', u'susan_pereira_', u'marie_heard_jan', u'martin_cuilla', u'david_delainey_jan', u'ryan_slinger_', u'john_lavorato_oct', u'mary_hain_', u'john_forney_', u'richard_sanders_oct', u'elizabeth_sager_nov', u'james_reitmeyer_nov', u'errol_mclaughlin_jun', u'louise_kitchen_mar', u'mark_taylor_jan', u'juan_hernandez_', u'energymarketplace', u'lindy_donoho_', u'tracy_geaccone', u'richard_sanders_dec', u'albert_meyers_', u'phillip_love_nov', u'elizabeth_sager_jan', u'invites_confirms', u'martin_cuilla_oct', u'fletcher_sturm_dec', u'barbo_', u'susan_pereira_nov', u'jason_wolfe', u'robin_rodrique_jun', u'fletcher_sturm_jun', u'richard_shapiro_june']\n"
     ]
    }
   ],
   "source": [
    "print freq_stopwords[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 835562 is out of bounds for axis 1 with size 835562",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b58c3e0e1718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create simple lda model (takes around 3700s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'lda generated in '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hweilun/anaconda2/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hweilun/anaconda2/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    671\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                     )\n\u001b[0;32m--> 673\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hweilun/anaconda2/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hweilun/anaconda2/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mexpElogbetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 835562 is out of bounds for axis 1 with size 835562"
     ]
    }
   ],
   "source": [
    "# create simple lda model (takes around 3700s)\n",
    "start = time()\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, update_every=1, chunksize=10000, passes=1)\n",
    "end = time()\n",
    "print 'lda generated in ',(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_saved\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(lda,open(\"lda_model.p\",\"wb\"))\n",
    "print 'model_saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.012*\"know\" + 0.011*\"get\" + 0.011*\"message\" + 0.009*\"would\" + 0.009*\"original\" + 0.009*\"time\" + 0.008*\"like\" + 0.008*\"let\" + 0.007*\"thanks\" + 0.006*\"one\"'),\n",
       " (1,\n",
       "  u'0.033*\"tls\" + 0.017*\"aw\" + 0.015*\"fx\" + 0.009*\"ig\" + 0.009*\"id\" + 0.006*\"gpia\" + 0.005*\"jb\" + 0.005*\"zw\" + 0.003*\"pz\" + 0.003*\"ly\"'),\n",
       " (2,\n",
       "  u'0.015*\"aw\" + 0.010*\"lc\" + 0.006*\"yb\" + 0.005*\"gb\" + 0.004*\"gr\" + 0.003*\"ig\" + 0.003*\"zsb\" + 0.002*\"gd\" + 0.002*\"yw\" + 0.002*\"cm\"'),\n",
       " (3,\n",
       "  u'0.123*\"enron\" + 0.117*\"ect\" + 0.061*\"hou\" + 0.027*\"pm\" + 0.022*\"corp\" + 0.021*\"forwarded\" + 0.018*\"ewc\" + 0.010*\"carol\" + 0.009*\"ees\" + 0.008*\"na\"'),\n",
       " (4,\n",
       "  u'0.035*\"enron_development\" + 0.019*\"broker\" + 0.017*\"wtg\" + 0.009*\"deal\" + 0.008*\"ees\" + 0.008*\"swap\" + 0.006*\"raptor\" + 0.006*\"ena\" + 0.005*\"enron\" + 0.005*\"ss\"'),\n",
       " (5,\n",
       "  u'0.018*\"enron\" + 0.008*\"new\" + 0.006*\"business\" + 0.005*\"availability\" + 0.005*\"site\" + 0.004*\"year\" + 0.004*\"one\" + 0.004*\"program\" + 0.004*\"time\" + 0.003*\"employees\"'),\n",
       " (6,\n",
       "  u'0.032*\"ptu\" + 0.028*\"kicagicagica\" + 0.027*\"ciagicagicagpe\" + 0.014*\"pc\" + 0.011*\"mda\" + 0.011*\"jq\" + 0.008*\"ju\" + 0.008*\"jt\" + 0.008*\"osu\" + 0.007*\"pgogicagicagidxptu\"'),\n",
       " (7,\n",
       "  u'0.010*\"power\" + 0.008*\"turbine\" + 0.008*\"would\" + 0.008*\"energy\" + 0.006*\"market\" + 0.005*\"company\" + 0.005*\"enron\" + 0.004*\"said\" + 0.004*\"state\" + 0.004*\"turbines\"'),\n",
       " (8,\n",
       "  u'0.052*\"thread\" + 0.040*\"mail\" + 0.034*\"version\" + 0.033*\"internet\" + 0.033*\"image\" + 0.030*\"microsoft\" + 0.030*\"index\" + 0.029*\"headers\" + 0.028*\"enron\" + 0.026*\"topic\"'),\n",
       " (9,\n",
       "  u'0.053*\"pm\" + 0.037*\"cn\" + 0.035*\"ou\" + 0.023*\"updatedby\" + 0.023*\"omnisubject\" + 0.023*\"omnistartdate\" + 0.023*\"omnistartdatetime\" + 0.023*\"omniorgtable\" + 0.023*\"omniform\" + 0.023*\"omni_viewicon\"'),\n",
       " (10,\n",
       "  u'0.015*\"aw\" + 0.010*\"yb\" + 0.007*\"tls\" + 0.006*\"grw\" + 0.005*\"vuc\" + 0.004*\"ic\" + 0.004*\"dy\" + 0.004*\"cy\" + 0.003*\"zw\" + 0.003*\"xpy\"'),\n",
       " (11,\n",
       "  u'0.058*\"font\" + 0.046*\"td\" + 0.025*\"size\" + 0.025*\"pm\" + 0.024*\"br\" + 0.023*\"pt\" + 0.021*\"nbsp\" + 0.018*\"tr\" + 0.017*\"outages\" + 0.016*\"scheduled\"'),\n",
       " (12,\n",
       "  u'0.029*\"gas\" + 0.014*\"prebon\" + 0.013*\"energy\" + 0.013*\"inc\" + 0.009*\"pl\" + 0.008*\"oil\" + 0.008*\"germany\" + 0.008*\"co\" + 0.007*\"canada\" + 0.006*\"amerex\"'),\n",
       " (13,\n",
       "  u'0.003*\"laguna\" + 0.003*\"kopie\" + 0.003*\"irena\" + 0.003*\"pts\" + 0.003*\"cgas\" + 0.002*\"neuner\" + 0.002*\"jarrod\" + 0.002*\"adm\" + 0.001*\"su\" + 0.001*\"boulanger\"'),\n",
       " (14,\n",
       "  u'0.228*\"com\" + 0.020*\"mail\" + 0.018*\"net\" + 0.017*\"aol\" + 0.013*\"hotmail\" + 0.008*\"yahoo\" + 0.007*\"doc\" + 0.007*\"org\" + 0.006*\"us\" + 0.005*\"jpg\"'),\n",
       " (15,\n",
       "  u'0.405*\"omni\" + 0.054*\"hou\" + 0.049*\"ect\" + 0.030*\"ou\" + 0.024*\"cn\" + 0.012*\"twt\" + 0.012*\"person\" + 0.010*\"pm\" + 0.007*\"alarm\" + 0.007*\"st\"'),\n",
       " (16,\n",
       "  u'0.025*\"aw\" + 0.010*\"ig\" + 0.007*\"zw\" + 0.005*\"zsb\" + 0.005*\"agf\" + 0.004*\"yw\" + 0.004*\"cm\" + 0.003*\"byb\" + 0.003*\"ugy\" + 0.003*\"jb\"'),\n",
       " (17,\n",
       "  u'0.018*\"nyiso\" + 0.017*\"email\" + 0.015*\"information\" + 0.015*\"may\" + 0.015*\"intended\" + 0.014*\"mail\" + 0.011*\"com\" + 0.010*\"pdf\" + 0.010*\"recipient\" + 0.010*\"message\"'),\n",
       " (18,\n",
       "  u'0.026*\"please\" + 0.012*\"thanks\" + 0.011*\"attached\" + 0.009*\"enron\" + 0.008*\"need\" + 0.008*\"know\" + 0.008*\"agreement\" + 0.007*\"questions\" + 0.007*\"sara\" + 0.007*\"information\"'),\n",
       " (19,\n",
       "  u'0.020*\"deal\" + 0.012*\"data\" + 0.011*\"deals\" + 0.009*\"price\" + 0.009*\"thanks\" + 0.007*\"day\" + 0.007*\"time\" + 0.007*\"month\" + 0.007*\"mw\" + 0.006*\"gas\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/hweilun/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-9a3d73f9e161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hweilun/anaconda2/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hweilun/anaconda2/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/hweilun/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "print stopwords.words('english')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
